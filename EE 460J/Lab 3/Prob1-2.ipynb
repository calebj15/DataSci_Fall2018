{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab #3\n",
    "\n",
    "# Caleb Johnson - cdj2273\n",
    "\n",
    "# Aimun Khan - aak2629\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1)\n",
    "# Brief Shannon Explanation\n",
    "In Shannon’s “A Mathematical Theory of Communication”, he explores the idea of compressing information using Markov models. Shannon gives useful examples to describe the various ways we might create randomly generated language. Drawing letters at random (zero order approximation) isn’t useful. Attaching probabilities to each letter based on frequency in language(first-order approximation) isn’t much better. A third option is that we might attach probabilities to letters dynamically, based on previous letters.(second/third order approximation).\n",
    "\n",
    "Expanding this idea to words, and using a second-order word approximation, Shannon found that chunks of words up to ten words in length formed sentence-like structure. This process leads naturally into the notions of choice, uncertainty, and entropy. Shannon was interested in finding a measure of “choice”. What he found was that the uncertainty, or entropy, of any given choice was equal to:\n",
    "\n",
    "Entropy = -∑(pi)*log(pi)\n",
    "\n",
    "In other words, the more evenly distributed the probabilities, the more uncertainty there will be in each choice. It makes intuitive sense. If all letters have 0 probability except the letter ‘e’, the system will predict ‘e’ each time and there will be no uncertainty/entropy in any decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections\n",
    "import sys\n",
    "import time, glob\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter#process_pdf\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from collections import Counter\n",
    "from cStringIO import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://gist.github.com/jmcarp/7105045\n",
    "#Could not get textract package installed, and pyPDF was ineffective\n",
    "\n",
    "def convert_pdf_to_txt(pdfname):\n",
    "\n",
    "    # PDFMiner boilerplate\n",
    "\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    sio = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, sio, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    # Extract text\n",
    "\n",
    "    fp = file(pdfname, 'rb')\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "\n",
    "    # Get text from StringIO\n",
    "\n",
    "    text = sio.getvalue()\n",
    "\n",
    "    # Cleanup\n",
    "    device.close()\n",
    "    sio.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_words(textfile, top=10):    \n",
    "\n",
    "    textfile = open(textfile)\n",
    "    text = textfile.read().lower()\n",
    "    textfile.close()\n",
    "    words = collections.Counter(text.split()) # how often each word appears\n",
    "\n",
    "    return dict(words.most_common(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#page_url = 'http://proceedings.mlr.press/v70/'\n",
    "#page = urllib.request.urlopen(page_url)\n",
    "#html = page.read().decode('utf-8')\n",
    "\n",
    "#soup = BeautifulSoup(html, 'html.parser')\n",
    "#papers = soup.find_all('div', {'class':'paper'})\n",
    "\n",
    "#pdf_links = []\n",
    "\n",
    "#for paper in papers:\n",
    "    #link = paper.find_all('a')[1]\n",
    "    #pdf_links.append(link['href'])\n",
    "    \n",
    "#for pdf_link in pdf_links:\n",
    "    #urllib.request.urlretrieve(pdf_link, 'pdfs/' + pdf_link.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find 10 most common words\n",
    "\n",
    "# I had trouble cleaning up the data, and decided to remove words of size 3 or smaller. After I figured out how to clean data properly, I forgot to remove this 3-letter restriction. Therefore, these are the 10 most common words of size 4 or greater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most common words are: {'from': 13949, 'learning': 13422, 'algorithm': 9541, 'that': 37464, 'this': 20714, 'where': 10566, 'have': 9363, 'which': 11571, 'model': 9276, 'with': 30374}\n"
     ]
    }
   ],
   "source": [
    "directory = 'C:\\\\Users\\\\caleb\\\\Desktop\\\\460J\\\\v70-gh-pages\\\\v70-gh-pages\\\\'\n",
    "dirs = os.listdir(directory)\n",
    "pdfs = []\n",
    "\n",
    "#for i in range(0, len(dirs)):\n",
    "    #num = os.listdir(directory+dirs[i])\n",
    "    #for j in range(0, len(num)):\n",
    "        #pdfs.append(directory + dirs[i] + '\\\\' + num[j])\n",
    "\n",
    "\n",
    "#for i in range(683, len(pdfs)):\n",
    "        #text = convert_pdf_to_txt(pdfs[i])\n",
    "        #text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "        #myfile = open(str(i)+'.txt', 'w')\n",
    "        #myfile.write(text)\n",
    "        #myfile.close()\n",
    "\n",
    "filenames = glob.glob('*.txt')\n",
    "outfilename = 'output'\n",
    "directory = 'C:\\\\Users\\\\caleb\\\\Desktop\\\\460J\\\\texts\\\\'\n",
    "dirs = os.listdir(directory)\n",
    "\n",
    "\n",
    "\n",
    "with open(outfilename, 'wb') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, 'r') as readfile:\n",
    "            infile = readfile.read()\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "            outfile.write(\"\\n\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "filename = 'output.txt'\n",
    "top_ten_words = find_most_common_words(filename, 10)\n",
    "\n",
    "print(\"The top 10 most common words are: \" + str(top_ten_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As shown above, the 10 most common words of size 4 or greater are (in order): \n",
    "1. 'that'\n",
    "2. 'with'\n",
    "3. 'this'\n",
    "4. 'from'\n",
    "5. 'learning'\n",
    "6. 'which'\n",
    "7. 'where'\n",
    "8. 'algorithm'\n",
    "9. 'have'\n",
    "10. 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>2077830</td>\n",
       "      <td>0.503973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>37135</td>\n",
       "      <td>0.009007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>29651</td>\n",
       "      <td>0.007192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>14353</td>\n",
       "      <td>0.003481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>13262</td>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      freq\n",
       "      2077830  0.503973\n",
       "that    37135  0.009007\n",
       "with    29651  0.007192\n",
       "this    14353  0.003481\n",
       "from    13262  0.003217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open(filename, \"r\") as word_list:\n",
    "    words = word_list.read().split(' ')\n",
    "s = pd.Series(words)\n",
    "vc = pd.DataFrame(s.value_counts())\n",
    "vc['freq'] = vc[0] / len(words)\n",
    "vc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of a randomly chosen word is: 6.956515533246443\n"
     ]
    }
   ],
   "source": [
    "vc['log'] = np.log2(vc['freq'])\n",
    "vc['entropy'] = vc['freq'] * vc['log']\n",
    "print(\"The entropy of a randomly chosen word is: \" + str(vc['entropy'].sum() * (-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print a randomly generated paragraph based on word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " measured true   Kucukelbir   Hado    summarize revie   performance  such Grant  makes replaced empirical with edge Figures  dataset algorithm telligence with   Statistics optimal  launched after   Bellemare much expression compared   requires group cross  iterations  paper Models  allows tributions  taking  Yoshua Huang model accuracy layers  with data hashing Analysis   database   replacing      Bertsekas   neighbor     derivative ularisation  with  abuse      Correspondence since     joint choosing   weyl  Model   honest approximate    repetitions    video Kvitkovicova   that during iterations further bounds    action parametrisation  Joint eval  Rosario    assume  classi  joint This  within  Neural  size with       parameters   method  evaluate   candidates Extension    Testing  Lloyd transitions     idea  classi  Wavenet examples SDPs Iteration computed rescalable accurate architectures each\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "sample = choice(vc.index, p =vc['freq'], size=200)\n",
    "with open('output1.txt', \"w\") as f:\n",
    "        print(\" \".join(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measured true   Kucukelbir   Hado    summarize revie   performance  such Grant  makes replaced empirical with edge Figures  dataset algorithm telligence with   Statistics optimal  launched after   Bellemare much expression compared   requires group cross  iterations  paper Models  allows tributions  taking  Yoshua Huang model accuracy layers  with data hashing Analysis   database   replacing      Bertsekas   neighbor     derivative ularisation  with  abuse      Correspondence since     joint choosing   weyl  Model   honest approximate    repetitions    video Kvitkovicova   that during iterations further bounds    action parametrisation  Joint eval  Rosario    assume  classi  joint This  within  Neural  size with       parameters   method  evaluate   candidates Extension    Testing  Lloyd transitions     idea  classi  Wavenet examples SDPs Iteration computed rescalable accurate architectures each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "bigram_words =[]\n",
    "for i in range(len(words) - 1):\n",
    "    bigram_words.append((words[i], words[i+1]))\n",
    "\n",
    "bigram_words\n",
    "\n",
    "bg = pd.Series(bigram_words)\n",
    "bpd = pd.DataFrame(bg.value_counts())\n",
    "bpd['freq'] = bpd[0] / bpd[0].sum()\n",
    "bpd[['word1','word2']] = bpd.index.to_series().apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateNextWord(word):\n",
    "    g = bpd.groupby('word1')\n",
    "    x = g.get_group(word)\n",
    "    x.sort_values('freq')\n",
    "    x['normfreq'] = x['freq'] / x['freq'].sum()\n",
    "    sample = choice(x['word2'], p =x['normfreq'], size=1)[0]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\Anaconda3\\envs\\Python 2.7\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference  this paper falls below Theorem  rewarded with  time    movie Hair spray Comedy Thriller   Cand   mode faster rate    used   vibrant  signal which cancel with respect  signed gradient    Bellman rank mcmcalpha rank approximation failures     compensate errors  speci       item  differentially private online   deep neural networks    Maji  architecture  SIGKDD international conference  estimate   emphasis each trajectory  order       Austerity  logarithmic time    stochastic weighted linear regression model importance scores  inte    above         with sentence   which requires   whole goal     control through circular shift  addi tion matrices under continual learning Neural Information Processing Systems Kudo Taku Kazawa Keith York Johnson Linden strauss  used Bregman divergence goes  that classi cation     satisfy  also know   other hand side here  Jason    Having     unit vectors    Assume  agent take  every function   built using  dataset includes standard assumptions which might   volume  archetypes from   mixtures General selection strategy  WiDi   trees displayed  Advances  running  Equation with convolutional neural network  least   Advances    pling scheme which   belongs  convolutional neural networks  experienced signi cant amount    amount  steps work   Uncini Aurelio    large clusters graph speci    linear models  zero coef cient solution   subset  good approximation    little overlap  where      assess  recent developments such  times   second  system   true gradient complexity   csie   sequence   cial DREAM challenge through extensive form   which means clustering model Efron Thisted have    have  algorithm    Localization   hierarchical probabilistic Lyapunov function   differ  Missing latent representations  harder  difference method   always   this change  alignment scores  Bach Jordan  with  each back  kernel Vish wanathan    sparse strong  Bernoulli  Zettlemoyer Luke       diagonal   given   independent  lower bound  following lower bound    units improve over binary tree whose  enable effective policies  International Conference  something rather than standard error  Details  Hessian approximations with  vate  task specific problems   interaction models  exploration  Riedmiller  bound  notion  carried  permutation group  times This implies that  always exists   have   possibly because  this challenge  waves while  plication   MCMC  reinforcement learning optimizers   spectral densities         competing methods Mathematical Program  effect   they have  where   depth resulting from labeled  concavity property Lemma Truncated squared error  have         exploited   difference between    history    Advances   regret Osband        vectors that  used        Wolfe algorithms   following         inclusion exclusion principle should choose           performance  meth   have that using standard property Unfortunately  sult  Schema Networks Algorithm except Abalone   encoder LSTM This also ubiquitous   formation      Hence    noise  more than   order methods include extensive form  every instance   output Personalization   lies  zero signed gradient  moreover  rank   Charlotte   consider    random  both tasks using  rewards     second layer  Then  Curran Associates      lett Nowak  needs  well even  some rows   video    Toronto Canada available     decomposed LSTM    Algorithm  each   follow from  Advances    glance this task  Unsupervised discovery     model complexity beyond   also achieves    follows   AAAI paper  Bubeck   Saul shows that   alleviating sampling estimation Support vector      development  terms inexact  bounded  convex cases involving   Select batch normalization      pair  following avor   Remarks Supervised learning  observed      lookahead search decoding  randomly  computed  Kulis Brian Vogel Jurafsky  Figure Error Bars Distributed Stochastic Click Models This implies that   nonparametric model  observations  together under           expectation  column   state space representation referred   distinctness  columbia   Louis  does   Section   training    global solution  initialized randomly   RKHS guarantees    then  sparse logistic regression  trix  precise phoneme conversion using articula tory  exactly    approximation error    same guar anteeing both graphs     uniformly only uses   Advances  matrix Discovering diverse structure  intervened variable associated eigenvalues  takes   random variables from         model Jalali  \n"
     ]
    }
   ],
   "source": [
    "bigram_gen = []\n",
    "sample = choice(bpd['word1'], p =bpd['freq'], size=1)[0]\n",
    "for i in range(1000):\n",
    "    n = generateNextWord(sample)\n",
    "    bigram_gen.append(n)\n",
    "    sample = n\n",
    "\n",
    "print(' '.join(bigram_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "inference  this paper falls below Theorem  rewarded with  time    movie Hair spray Comedy Thriller   Cand   mode faster rate    used   vibrant  signal which cancel with respect  signed gradient    Bellman rank mcmcalpha rank approximation failures     compensate errors  speci       item  differentially private online   deep neural networks    Maji  architecture  SIGKDD international conference  estimate   emphasis each trajectory  order       Austerity  logarithmic time    stochastic weighted linear regression model importance scores  inte    above         with sentence   which requires   whole goal     control through circular shift  addi tion matrices under continual learning Neural Information Processing Systems Kudo Taku Kazawa Keith York Johnson Linden strauss  used Bregman divergence goes  that classi cation     satisfy  also know   other hand side here  Jason    Having     unit vectors    Assume  agent take  every function   built using  dataset includes standard assumptions which might   volume  archetypes from   mixtures General selection strategy  WiDi   trees displayed  Advances  running  Equation with convolutional neural network  least   Advances    pling scheme which   belongs  convolutional neural networks  experienced signi cant amount    amount  steps work   Uncini Aurelio    large clusters graph speci    linear models  zero coef cient solution   subset  good approximation    little overlap  where      assess  recent developments such  times   second  system   true gradient complexity   csie   sequence   cial DREAM challenge through extensive form   which means clustering model Efron Thisted have    have  algorithm    Localization   hierarchical probabilistic Lyapunov function   differ  Missing latent representations  harder  difference method   always   this change  alignment scores  Bach Jordan  with  each back  kernel Vish wanathan    sparse strong  Bernoulli  Zettlemoyer Luke       diagonal   given   independent  lower bound  following lower bound    units improve over binary tree whose  enable effective policies  International Conference  something rather than standard error  Details  Hessian approximations with  vate  task specific problems   interaction models  exploration  Riedmiller  bound  notion  carried  permutation group  times This implies that  always exists   have   possibly because  this challenge  waves while  plication   MCMC  reinforcement learning optimizers   spectral densities         competing methods Mathematical Program  effect   they have  where   depth resulting from labeled  concavity property Lemma Truncated squared error  have         exploited   difference between    history    Advances   regret Osband        vectors that  used        Wolfe algorithms   following         inclusion exclusion principle should choose           performance  meth   have that using standard property Unfortunately  sult  Schema Networks Algorithm except Abalone   encoder LSTM This also ubiquitous   formation      Hence    noise  more than   order methods include extensive form  every instance   output Personalization   lies  zero signed gradient  moreover  rank   Charlotte   consider    random  both tasks using  rewards     second layer  Then  Curran Associates      lett Nowak  needs  well even  some rows   video    Toronto Canada available     decomposed LSTM    Algorithm  each   follow from  Advances    glance this task  Unsupervised discovery     model complexity beyond   also achieves    follows   AAAI paper  Bubeck   Saul shows that   alleviating sampling estimation Support vector      development  terms inexact  bounded  convex cases involving   Select batch normalization      pair  following avor   Remarks Supervised learning  observed      lookahead search decoding  randomly  computed  Kulis Brian Vogel Jurafsky  Figure Error Bars Distributed Stochastic Click Models This implies that   nonparametric model  observations  together under           expectation  column   state space representation referred   distinctness  columbia   Louis  does   Section   training    global solution  initialized randomly   RKHS guarantees    then  sparse logistic regression  trix  precise phoneme conversion using articula tory  exactly    approximation error    same guar anteeing both graphs     uniformly only uses   Advances  matrix Discovering diverse structure  intervened variable associated eigenvalues  takes   random variables from         model Jalali"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
