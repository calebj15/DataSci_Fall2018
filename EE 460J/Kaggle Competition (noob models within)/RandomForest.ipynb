{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_final.csv')\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "Y = data.Y\n",
    "X = pd.DataFrame(data.drop('Id', axis=1))\n",
    "X = X.drop('Y', axis=1)\n",
    "\n",
    "new = pd.DataFrame({'f24': ['f25', 'f26', 'f27','f28','f29','f30','f31']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f24'], prefix='f24')],axis=1)\n",
    "X.drop(['f24'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f20': ['f32', 'f33', 'f34','f35','f36','f37','f38', 'f39']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f20'], prefix='f20')],axis=1)\n",
    "X.drop(['f20'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f11': ['f40', 'f41', 'f42','f43','f44','f45','f46']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f11'], prefix='f11')],axis=1)\n",
    "X.drop(['f11'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f6': ['f47', 'f48', 'f49','f50','f51','f52','f53', 'f54', 'f55']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f6'], prefix='f6')],axis=1)\n",
    "X.drop(['f6'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f18': ['f56', 'f57', 'f58','f59','f60','f61','f62', 'f63', 'f64']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f18'], prefix='f18')],axis=1)\n",
    "X.drop(['f18'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f9': ['f65', 'f66', 'f67','f68','f69','f70','f71',]})\n",
    "X = pd.concat([X,pd.get_dummies(X['f9'], prefix='f9')],axis=1)\n",
    "X.drop(['f9'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f5': ['f72', 'f73', 'f74','f75','f76','f77','f78', 'f79']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f5'], prefix='f5')],axis=1)\n",
    "X.drop(['f5'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f22': ['f80', 'f81', 'f82','f83','f84','f85','f86']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f22'], prefix='f22')],axis=1)\n",
    "X.drop(['f22'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f2': ['f87', 'f88', 'f89','f90','f91','f92','f93']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f2'], prefix='f2')],axis=1)\n",
    "X.drop(['f2'],axis=1, inplace=True)\n",
    "\n",
    "new = pd.DataFrame({'f21': ['f94', 'f95', 'f96','f97','f98','f99','f100', 'f101', 'f102']})\n",
    "X = pd.concat([X,pd.get_dummies(X['f21'], prefix='f21')],axis=1)\n",
    "X.drop(['f21'],axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "[X.iloc[i, j] for i,j in zip(*np.where(pd.isnull(X)))]\n",
    "X.to_csv(\"RF one hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature ranking:\n",
    "1. feature 13 (0.630658)\n",
    "2. feature 14 (0.049901)\n",
    "3. feature 0 (0.046843)\n",
    "4. feature 15 (0.039436)\n",
    "5. feature 7 (0.035876)\n",
    "6. feature 3 (0.031193)\n",
    "7. feature 22 (0.022897)\n",
    "8. feature 12 (0.019104)\n",
    "9. feature 16 (0.017205)\n",
    "10. feature 2 (0.015941)\n",
    "11. feature 18 (0.015446)\n",
    "12. feature 6 (0.014328)\n",
    "13. feature 9 (0.012948)\n",
    "14. feature 11 (0.012947)\n",
    "15. feature 20 (0.006352)\n",
    "16. feature 1 (0.005217)\n",
    "17. feature 21 (0.004523)\n",
    "18. feature 4 (0.003571)\n",
    "19. feature 8 (0.003420)\n",
    "20. feature 17 (0.003290)\n",
    "21. feature 5 (0.002698)\n",
    "22. feature 10 (0.002642)\n",
    "23. feature 19 (0.002080)\n",
    "24. feature 23 (0.001484)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10, 20, 30], 'min_samples_leaf': [1, 2, 4, 25, 50]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 20, 30]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 25, 50]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              }\n",
    "\n",
    "print(random_grid)\n",
    "values = {'nan': 0}\n",
    "X.fillna(value=values)\n",
    "[X.iloc[i, j] for i,j in zip(*np.where(pd.isnull(X)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   50.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 46.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10, 20, 30], 'min_samples_leaf': [1, 2, 4, 25, 50]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 30,\n",
       " 'min_samples_leaf': 25,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 70}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=70,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=25, min_samples_split=30,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = RandomForestRegressor(n_estimators=800, min_samples_split=30, min_samples_leaf=25, max_features='auto', max_depth=70, bootstrap=True)\n",
    "best_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 8 (0.776411)\n",
      "2. feature 9 (0.036729)\n",
      "3. feature 0 (0.030682)\n",
      "4. feature 10 (0.025675)\n",
      "5. feature 2 (0.024659)\n",
      "6. feature 4 (0.024086)\n",
      "7. feature 11 (0.015208)\n",
      "8. feature 13 (0.014731)\n",
      "9. feature 7 (0.013175)\n",
      "10. feature 12 (0.010460)\n",
      "11. feature 3 (0.009788)\n",
      "12. feature 1 (0.006232)\n",
      "13. feature 5 (0.006073)\n",
      "14. feature 6 (0.004204)\n",
      "15. feature 82 (0.000280)\n",
      "16. feature 75 (0.000267)\n",
      "17. feature 36 (0.000197)\n",
      "18. feature 68 (0.000185)\n",
      "19. feature 54 (0.000140)\n",
      "20. feature 44 (0.000122)\n",
      "21. feature 76 (0.000121)\n",
      "22. feature 15 (0.000120)\n",
      "23. feature 61 (0.000078)\n",
      "24. feature 53 (0.000069)\n",
      "25. feature 69 (0.000059)\n",
      "26. feature 35 (0.000057)\n",
      "27. feature 83 (0.000052)\n",
      "28. feature 45 (0.000045)\n",
      "29. feature 29 (0.000043)\n",
      "30. feature 28 (0.000026)\n",
      "31. feature 60 (0.000017)\n",
      "32. feature 14 (0.000009)\n",
      "33. feature 22 (0.000003)\n",
      "34. feature 78 (0.000000)\n",
      "35. feature 20 (0.000000)\n",
      "36. feature 87 (0.000000)\n",
      "37. feature 86 (0.000000)\n",
      "38. feature 85 (0.000000)\n",
      "39. feature 27 (0.000000)\n",
      "40. feature 26 (0.000000)\n",
      "41. feature 25 (0.000000)\n",
      "42. feature 24 (0.000000)\n",
      "43. feature 23 (0.000000)\n",
      "44. feature 21 (0.000000)\n",
      "45. feature 84 (0.000000)\n",
      "46. feature 77 (0.000000)\n",
      "47. feature 19 (0.000000)\n",
      "48. feature 18 (0.000000)\n",
      "49. feature 17 (0.000000)\n",
      "50. feature 16 (0.000000)\n",
      "51. feature 81 (0.000000)\n",
      "52. feature 31 (0.000000)\n",
      "53. feature 73 (0.000000)\n",
      "54. feature 80 (0.000000)\n",
      "55. feature 74 (0.000000)\n",
      "56. feature 79 (0.000000)\n",
      "57. feature 30 (0.000000)\n",
      "58. feature 34 (0.000000)\n",
      "59. feature 32 (0.000000)\n",
      "60. feature 33 (0.000000)\n",
      "61. feature 67 (0.000000)\n",
      "62. feature 66 (0.000000)\n",
      "63. feature 65 (0.000000)\n",
      "64. feature 64 (0.000000)\n",
      "65. feature 63 (0.000000)\n",
      "66. feature 62 (0.000000)\n",
      "67. feature 59 (0.000000)\n",
      "68. feature 58 (0.000000)\n",
      "69. feature 57 (0.000000)\n",
      "70. feature 56 (0.000000)\n",
      "71. feature 55 (0.000000)\n",
      "72. feature 71 (0.000000)\n",
      "73. feature 52 (0.000000)\n",
      "74. feature 51 (0.000000)\n",
      "75. feature 50 (0.000000)\n",
      "76. feature 49 (0.000000)\n",
      "77. feature 48 (0.000000)\n",
      "78. feature 47 (0.000000)\n",
      "79. feature 46 (0.000000)\n",
      "80. feature 88 (0.000000)\n",
      "81. feature 43 (0.000000)\n",
      "82. feature 42 (0.000000)\n",
      "83. feature 41 (0.000000)\n",
      "84. feature 40 (0.000000)\n",
      "85. feature 39 (0.000000)\n",
      "86. feature 38 (0.000000)\n",
      "87. feature 37 (0.000000)\n",
      "88. feature 72 (0.000000)\n",
      "89. feature 70 (0.000000)\n",
      "90. feature 89 (0.000000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-616baf26031f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Plot the feature importances of the forest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Feature importances\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m plt.bar(range(X.shape[1]), importances[indices],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "importances = best_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785708901269847"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "yhat = best_model.predict(X_train)\n",
    "yt = y_train\n",
    "score = roc_auc_score(yt, yhat)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8070112554894501"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = best_model.predict(X_test)\n",
    "yt = y_test\n",
    "score = roc_auc_score(yt, yhat)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pd.DataFrame(pd.read_csv('test_final.csv'))\n",
    "x_test = x_test.drop('Id', 1)\n",
    "x_test = x_test.drop('f24', axis=1)\n",
    "x_test = x_test.drop('f20', axis=1)\n",
    "x_test = x_test.drop('f11', axis=1)\n",
    "x_test = x_test.drop('f6', axis=1)\n",
    "x_test = x_test.drop('f18', axis=1)\n",
    "x_test = x_test.drop('f9', axis=1)\n",
    "x_test = x_test.drop('f5', axis=1)\n",
    "x_test = x_test.drop('f22', axis=1)\n",
    "x_test = x_test.drop('f2', axis=1)\n",
    "x_test = x_test.drop('f21', axis=1)\n",
    "\n",
    "\n",
    "yhat = best_model.predict(x_test)\n",
    "yhat = pd.DataFrame(yhat)\n",
    "    \n",
    "yhat.to_csv(\"RF_Day2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.924562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.869870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.989352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.922422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.873613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.957952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.983449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.921647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.948564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.975224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.946902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.898831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.983615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.947494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.966628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.962672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.941095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.945606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.950998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.953365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.904319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16355</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16356</th>\n",
       "      <td>0.961928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16357</th>\n",
       "      <td>0.961634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16358</th>\n",
       "      <td>0.936571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16359</th>\n",
       "      <td>0.808376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16360</th>\n",
       "      <td>0.962168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16361</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16362</th>\n",
       "      <td>0.908426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>0.963235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16364</th>\n",
       "      <td>0.925973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16365</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16366</th>\n",
       "      <td>0.951523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16367</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16368</th>\n",
       "      <td>0.942815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16369</th>\n",
       "      <td>0.980844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16370</th>\n",
       "      <td>0.962851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16371</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16372</th>\n",
       "      <td>0.920151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>0.892968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16374</th>\n",
       "      <td>0.924737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>0.935616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16376</th>\n",
       "      <td>0.946469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16377</th>\n",
       "      <td>0.974614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16378</th>\n",
       "      <td>0.987293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16379</th>\n",
       "      <td>0.978782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16380</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16381</th>\n",
       "      <td>0.929962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16382</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16383</th>\n",
       "      <td>0.963509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16384</th>\n",
       "      <td>0.895595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16385 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.924562\n",
       "1      0.869870\n",
       "2      1.000000\n",
       "3      1.000000\n",
       "4      0.989352\n",
       "5      1.000000\n",
       "6      0.922422\n",
       "7      0.873613\n",
       "8      1.000000\n",
       "9      1.000000\n",
       "10     0.957952\n",
       "11     0.983449\n",
       "12     1.000000\n",
       "13     0.921647\n",
       "14     0.948564\n",
       "15     1.000000\n",
       "16     0.975224\n",
       "17     1.000000\n",
       "18     0.946902\n",
       "19     0.898831\n",
       "20     0.983615\n",
       "21     0.947494\n",
       "22     0.966628\n",
       "23     0.962672\n",
       "24     1.000000\n",
       "25     0.941095\n",
       "26     0.945606\n",
       "27     0.950998\n",
       "28     0.953365\n",
       "29     0.904319\n",
       "...         ...\n",
       "16355  1.000000\n",
       "16356  0.961928\n",
       "16357  0.961634\n",
       "16358  0.936571\n",
       "16359  0.808376\n",
       "16360  0.962168\n",
       "16361  1.000000\n",
       "16362  0.908426\n",
       "16363  0.963235\n",
       "16364  0.925973\n",
       "16365  1.000000\n",
       "16366  0.951523\n",
       "16367  1.000000\n",
       "16368  0.942815\n",
       "16369  0.980844\n",
       "16370  0.962851\n",
       "16371  1.000000\n",
       "16372  0.920151\n",
       "16373  0.892968\n",
       "16374  0.924737\n",
       "16375  0.935616\n",
       "16376  0.946469\n",
       "16377  0.974614\n",
       "16378  0.987293\n",
       "16379  0.978782\n",
       "16380  1.000000\n",
       "16381  0.929962\n",
       "16382  1.000000\n",
       "16383  0.963509\n",
       "16384  0.895595\n",
       "\n",
       "[16385 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
